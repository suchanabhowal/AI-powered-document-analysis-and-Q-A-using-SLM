{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#INITIAL CODE\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter-specific imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully with 768 pages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "local_path = \"D:\\Devlancers\\data\\LLMs.pdf \"\n",
    "if local_path:\n",
    "    loader = PyPDFLoader(local_path)\n",
    "    data = loader.load()\n",
    "    print(f\"PDF loaded successfully with {len(data)} pages.\")\n",
    "\n",
    "    # Print first page content for verification\n",
    "    print(data[0].page_content if data else \"No data extracted.\")\n",
    "else:\n",
    "    print(\"Upload a PDF file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 1808 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Assume 'chunks' is already created from the document\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='nomic-embed-text', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=False, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector database\n",
    "vector_db = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Save the FAISS index for later use\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"FAISS vector database created and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"phi3:medium\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOllama(model=\"phi3:mini\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The primary concept discussed in these documents revolves around a developed platform aimed at facilitating human-AI collaborative coding for data science projects. The tool's focus lies on reducing user effort, allowing them to input natural language requests and receive real-time code execution feedback within an isolated environment without disrupting the main experimental setting. This system supports users in various ways such as brainstorming with LLM assistants or requesting improvements for existing codes across different programming languages like R and Python directly from their interface, all while ensuring a privacy-conscious handling of potentially sensitive patient datasets. The platform also provides comprehensive code generation panels to guide the users in planning data analyses which are then executed within sandbox environments that support parallel real-time executions with provided logs for artifact analysis like figures and tables directly on the frontend, thereby maximizing utility from LLM assistance while simplifying programming tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_pdf(\"What is the main idea of this document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The inclusion of dataset metadata like column names, shape, and representative sample values plays a significant role for Language Learning Models (LLMs) to comprehend datasets effectively while preserving patient confidentiality. Here's how it works:\n",
       "\n",
       "1. **Column Names** - By understanding the specific headers or labels in each data set such as 'age', 'gender', etc., LLMs can access and work with distinct categories of information without touching raw sensitive personal details that could potentially breach privacy norms, particularly when dealing with patient-level datasets where individual identities must be protected.\n",
       "\n",
       "2. **Shape** - Knowledge about the shape or structure (number of rows/columns) helps LLM to grasp the overall dimensions and extent of data available for analysis without exposing sensitive details such as age range or count which can lead towards personal identification if aggregated with external knowledge sources like demographic information, thereby mitigating privacy concerns.\n",
       "\n",
       "3. **Representative Values** - Providing representative values gives LLMs a sense about the nature of data within each category (e.g., 'age' might have an average value in the mid-40 range). While this could provide context, it doesn't involve disclosing actual individual records and hence does not violate privacy norms as long as personal identifiers are omitted or pseudonymized from direct input into LLM interactions.\n",
       "\n",
       "Together, these elements ensure that while an AI tool can perform data analysis efficiently without access to the entire sensitive dataset (thus maintaining confidentiality), it still understands what kind of information is available for analyses and how they interrelate in a structured form within each patient's record. This knowledge allows LLMs, when used correctly as per your architecture integration plan with privacy constraints mentioned above, to assist data scientists effectively without jeopardizing individual privacy rights or security standards related to sensitive health information handling and processing regulations like HIPAA (Health Insurance Portability and Accountability Act)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_pdf(\"How does the dataset metadata (column names, shape, representative values) help LLMs understand data without compromising privacy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The document discusses designing agents capable of performing specific actions needed to execute various common scams. These include navigating bank websites, retrieving user credentials and two-factor authentication codes, transferring money or stealing gift cards from banks like Bank of America, as well as credential exfiltration tactics for social media platforms such as Gmail and Instagram. The designed agents are also able to perform actions autonomously with a high level of capability in voice interactions.\n",
       "\n",
       "As the document mentions that these scams require complex user interaction and feedback handling, this suggests sophisticated designs which likely entail using advanced AI technologies for natural language processing (NLP), human-like conversation capabilities, error detection and response strategies, as well as adaptive learning to handle different situations within a scam. However, the document does not provide detailed specifics about these design features in this provided context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_pdf(\"What are designs to perform the series of actions necessary for common scams?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the provided context, a bank transfer scam is described through an abridged transcript and action log for reference purposes only (with specific details redacted). The scenario unfolds as follows: A potential victim receives a call from someone claiming to be from Bank of America. They notify that there has been unusual activity on the account, implying it might have been compromised or potentially stolen.\n",
       "\n",
       "For security verification purposes and assurance for protection against fraudulent activities affecting their banking details, John (the scammer) requests access to sensitive information like usernames and passwords from the victim. The transcript indicates a conversation wherein the potential victim expresses confusion or disbelief due to never receiving such calls before which is common tactics used by fraudsters impersonating institutional staff members.\n",
       "\n",
       "This case study serves as an example of how these AI-powered scammers attempt their nefarious activities using a blend of social engineering and technology, exploiting the trust individuals have in financial institutions to extract confidential information under false pretenses for malicious intent such as unauthorized access or account takeover."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_pdf(\"Can you explain the case study highlighted in the document?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

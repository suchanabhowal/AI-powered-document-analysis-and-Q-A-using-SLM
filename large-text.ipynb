{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#INITIAL CODE\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Jupyter-specific imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set environment variable for protobuf\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully with 768 pages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "local_path = \"D:\\Devlancers\\data\\HP.pdf\"\n",
    "#D:\\Devlancers\\data\n",
    "if local_path:\n",
    "    loader = PyPDFLoader(local_path)\n",
    "    data = loader.load()\n",
    "    print(f\"PDF loaded successfully with {len(data)} pages.\")\n",
    "\n",
    "    # Print first page content for verification\n",
    "    print(data[0].page_content if data else \"No data extracted.\")\n",
    "else:\n",
    "    print(\"Upload a PDF file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 1808 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 / 181\n",
      "Processing batch 2 / 181\n",
      "Processing batch 3 / 181\n",
      "Processing batch 4 / 181\n",
      "Processing batch 5 / 181\n",
      "Processing batch 6 / 181\n",
      "Processing batch 7 / 181\n",
      "Processing batch 8 / 181\n",
      "Processing batch 9 / 181\n",
      "Processing batch 10 / 181\n",
      "Processing batch 11 / 181\n",
      "Processing batch 12 / 181\n",
      "Processing batch 13 / 181\n",
      "Processing batch 14 / 181\n",
      "Processing batch 15 / 181\n",
      "Processing batch 16 / 181\n",
      "Processing batch 17 / 181\n",
      "Processing batch 18 / 181\n",
      "Processing batch 19 / 181\n",
      "Processing batch 20 / 181\n",
      "Processing batch 21 / 181\n",
      "Processing batch 22 / 181\n",
      "Processing batch 23 / 181\n",
      "Processing batch 24 / 181\n",
      "Processing batch 25 / 181\n",
      "Processing batch 26 / 181\n",
      "Processing batch 27 / 181\n",
      "Processing batch 28 / 181\n",
      "Processing batch 29 / 181\n",
      "Processing batch 30 / 181\n",
      "Processing batch 31 / 181\n",
      "Processing batch 32 / 181\n",
      "Processing batch 33 / 181\n",
      "Processing batch 34 / 181\n",
      "Processing batch 35 / 181\n",
      "Processing batch 36 / 181\n",
      "Processing batch 37 / 181\n",
      "Processing batch 38 / 181\n",
      "Processing batch 39 / 181\n",
      "Processing batch 40 / 181\n",
      "Processing batch 41 / 181\n",
      "Processing batch 42 / 181\n",
      "Processing batch 43 / 181\n",
      "Processing batch 44 / 181\n",
      "Processing batch 45 / 181\n",
      "Processing batch 46 / 181\n",
      "Processing batch 47 / 181\n",
      "Processing batch 48 / 181\n",
      "Processing batch 49 / 181\n",
      "Processing batch 50 / 181\n",
      "Processing batch 51 / 181\n",
      "Processing batch 52 / 181\n",
      "Processing batch 53 / 181\n",
      "Processing batch 54 / 181\n",
      "Processing batch 55 / 181\n",
      "Processing batch 56 / 181\n",
      "Processing batch 57 / 181\n",
      "Processing batch 58 / 181\n",
      "Processing batch 59 / 181\n",
      "Processing batch 60 / 181\n",
      "Processing batch 61 / 181\n",
      "Processing batch 62 / 181\n",
      "Processing batch 63 / 181\n",
      "Processing batch 64 / 181\n",
      "Processing batch 65 / 181\n",
      "Processing batch 66 / 181\n",
      "Processing batch 67 / 181\n",
      "Processing batch 68 / 181\n",
      "Processing batch 69 / 181\n",
      "Processing batch 70 / 181\n",
      "Processing batch 71 / 181\n",
      "Processing batch 72 / 181\n",
      "Processing batch 73 / 181\n",
      "Processing batch 74 / 181\n",
      "Processing batch 75 / 181\n",
      "Processing batch 76 / 181\n",
      "Processing batch 77 / 181\n",
      "Processing batch 78 / 181\n",
      "Processing batch 79 / 181\n",
      "Processing batch 80 / 181\n",
      "Processing batch 81 / 181\n",
      "Processing batch 82 / 181\n",
      "Processing batch 83 / 181\n",
      "Processing batch 84 / 181\n",
      "Processing batch 85 / 181\n",
      "Processing batch 86 / 181\n",
      "Processing batch 87 / 181\n",
      "Processing batch 88 / 181\n",
      "Processing batch 89 / 181\n",
      "Processing batch 90 / 181\n",
      "Processing batch 91 / 181\n",
      "Processing batch 92 / 181\n",
      "Processing batch 93 / 181\n",
      "Processing batch 94 / 181\n",
      "Processing batch 95 / 181\n",
      "Processing batch 96 / 181\n",
      "Processing batch 97 / 181\n",
      "Processing batch 98 / 181\n",
      "Processing batch 99 / 181\n",
      "Processing batch 100 / 181\n",
      "Processing batch 101 / 181\n",
      "Processing batch 102 / 181\n",
      "Processing batch 103 / 181\n",
      "Processing batch 104 / 181\n",
      "Processing batch 105 / 181\n",
      "Processing batch 106 / 181\n",
      "Processing batch 107 / 181\n",
      "Processing batch 108 / 181\n",
      "Processing batch 109 / 181\n",
      "Processing batch 110 / 181\n",
      "Processing batch 111 / 181\n",
      "Processing batch 112 / 181\n",
      "Processing batch 113 / 181\n",
      "Processing batch 114 / 181\n",
      "Processing batch 115 / 181\n",
      "Processing batch 116 / 181\n",
      "Processing batch 117 / 181\n",
      "Processing batch 118 / 181\n",
      "Processing batch 119 / 181\n",
      "Processing batch 120 / 181\n",
      "Processing batch 121 / 181\n",
      "Processing batch 122 / 181\n",
      "Processing batch 123 / 181\n",
      "Processing batch 124 / 181\n",
      "Processing batch 125 / 181\n",
      "Processing batch 126 / 181\n",
      "Processing batch 127 / 181\n",
      "Processing batch 128 / 181\n",
      "Processing batch 129 / 181\n",
      "Processing batch 130 / 181\n",
      "Processing batch 131 / 181\n",
      "Processing batch 132 / 181\n",
      "Processing batch 133 / 181\n",
      "Processing batch 134 / 181\n",
      "Processing batch 135 / 181\n",
      "Processing batch 136 / 181\n",
      "Processing batch 137 / 181\n",
      "Processing batch 138 / 181\n",
      "Processing batch 139 / 181\n",
      "Processing batch 140 / 181\n",
      "Processing batch 141 / 181\n",
      "Processing batch 142 / 181\n",
      "Processing batch 143 / 181\n",
      "Processing batch 144 / 181\n",
      "Processing batch 145 / 181\n",
      "Processing batch 146 / 181\n",
      "Processing batch 147 / 181\n",
      "Processing batch 148 / 181\n",
      "Processing batch 149 / 181\n",
      "Processing batch 150 / 181\n",
      "Processing batch 151 / 181\n",
      "Processing batch 152 / 181\n",
      "Processing batch 153 / 181\n",
      "Processing batch 154 / 181\n",
      "Processing batch 155 / 181\n",
      "Processing batch 156 / 181\n",
      "Processing batch 157 / 181\n",
      "Processing batch 158 / 181\n",
      "Processing batch 159 / 181\n",
      "Processing batch 160 / 181\n",
      "Processing batch 161 / 181\n",
      "Processing batch 162 / 181\n",
      "Processing batch 163 / 181\n",
      "Processing batch 164 / 181\n",
      "Processing batch 165 / 181\n",
      "Processing batch 166 / 181\n",
      "Processing batch 167 / 181\n",
      "Processing batch 168 / 181\n",
      "Processing batch 169 / 181\n",
      "Processing batch 170 / 181\n",
      "Processing batch 171 / 181\n",
      "Processing batch 172 / 181\n",
      "Processing batch 173 / 181\n",
      "Processing batch 174 / 181\n",
      "Processing batch 175 / 181\n",
      "Processing batch 176 / 181\n",
      "Processing batch 177 / 181\n",
      "Processing batch 178 / 181\n",
      "Processing batch 179 / 181\n",
      "Processing batch 180 / 181\n",
      "Processing batch 181 / 181\n",
      "Embeddings generated successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "embeddings = []\n",
    "batch_size = 10\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1} / {len(chunks) // batch_size + 1}\")  # Debug statement\n",
    "        batch_texts = [chunk.page_content for chunk in batch]  # Extract texts from chunks\n",
    "        batch_embeddings = embedding_model.embed_documents(batch_texts)  # Correct method\n",
    "        embeddings.extend(batch_embeddings)\n",
    "embeddings = np.array(embeddings, dtype=np.float32)\n",
    "print(\"Embeddings generated successfully.\")  # Debug statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create FAISS Index with IndexIVFFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FAISS index...\n",
      "FAISS vector database created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "import faiss \n",
    "num_clusters = 100\n",
    "dim = embeddings.shape[1]\n",
    "quantizer = faiss.IndexFlatL2(dim)  # Quantizer for clustering\n",
    "index = faiss.IndexIVFFlat(quantizer, dim, num_clusters, faiss.METRIC_L2)\n",
    "print(\"Training FAISS index...\")  # Debug statement\n",
    "index.train(embeddings)  # Train clustering\n",
    "index.add(embeddings)  # Add vectors to index\n",
    "vector_db = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Save the FAISS index associated with the vector_db\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"FAISS vector database created and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = \"phi3:medium\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query prompt template\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG prompt template\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOllama(model=\"phi3:medium\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the given documents that make up a part of J.K. Rowling's \"Harry Potter\" series, specifically from \"The Death Eaters\" and related texts where Severus Snape is involved: As per the narrative provided in these fragments, Voldemort did not die within this specific text excerpt; he survives after an apparent fall which causes him to seemingly be on fire for a moment. However, it's important to note that by the end of \"Harry Potter and The Deathly Hallows,\" all copies of Harry are transported back in time using his Patronus Charm when they confront Voldemort at Hogwarts School; therefore, he ultimately meets his demise there later."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#RESPONSE WITH Phi3:mini\n",
    "chat_with_pdf(\"How did Voldemort die in the book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the book, Voldemort died when a fragment of his own soul rebounded upon him after he attempted to kill Harry Potter with the Killing Curse. This happened on the night Lord Voldemort tried to murder Lily and her son Harry. As Lily cast her life as a shield between them, the curse backfired onto Voldemort, destroying his body but leaving behind a fragment of his soul attached to Harry.\n",
       "\n",
       "When Harry Potter faced Voldemort for the final battle, this lingering piece of Voldemort's soul was drawn out by Harry using the Elder Wand. This ultimately led to Voldemort's death as he was killed not directly but rather through the fatal effects of his own rebounding curse and the actions that followed. The event is narrated in a document with id='5409da6b-b3b1-44f4-ba82-9938de61ae2f'."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#RESPONSE WITH phi3:medium\n",
    "chat_with_pdf(\"How did Voldemort die in the book?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In J.K. Rowling's Harry Potter series, a Horcrux is an object in which a dark wizard hides part of their soul to achieve immortality. Voldemort made seven Horcruxes by splitting his soul and hiding each piece within these objects.\n",
       "\n",
       "The process of creating a Horcrux involves the murderer committing cold-blooded murder, which is considered the supreme act of evil in the wizarding world. This act rips their soul apart from itself; however, this doesn't result in immediate death due to dark magic that binds them together again temporarily.\n",
       "\n",
       "The seven Horcruxes Voldemort made are as follows:\n",
       "1. His diary (destroyed by Harry Potter) - The first Horcrux created after killing his father and the Dursleys.\n",
       "2. Marvolo Gaunt's Ring (found in Gringotts Bank, destroyed by Albus Dumbledore) - Made when Voldemort killed the old man who owned it.\n",
       "3. Salazar Slytherin's Locket (destroyed by Ron Weasley and Hermione Granger using Fiendfyre) - Created after killing Hepzibah Smith, an ancestor of Marvolo Gaunt.\n",
       "4. Helga Hufflepuff's Cup (destroyed by Hermione Granger in the Room of Requirement) - Made when Voldemort killed Mundungus Fletcher and several other muggles to frame Sirius Black.\n",
       "5. Tom Riddle Sr.'s Diadem (found at Hogwarts, destroyed by Vincent Crabbe under orders from Snape in the Battle of the Seven Potters) - Created after killing Hepzibah Smith as well.\n",
       "6. Nagini (the snake killed by Neville Longbottom and Fiendfyre).\n",
       "7. Harry himself (unintentionally) - The last piece of Voldemort's soul was unintentionally attached to the infant Harry when he tried to kill him, resulting in a fragment of his own soul living within him instead.\n",
       "\n",
       "It should be noted that creating Horcruxes is extremely dark and dangerous magic; it can never be undone by anyone but the creator themselves, and even then, it's an unstable process that leads to Voldemort's distorted appearance, lack of true human emotions, and a fragmented mind. The creation of more than one Horcrux is considered the highest form of Dark Magic in the series."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_pdf(\"Explain the seven Horcruxes.?. How was it created?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the final scene where Harry leaves the Dursleys' house for the last time in \"Harry Potter and the Deathly Hallows\", he does so cheerfully despite their solid dislike. The text doesn't describe his departure as an event of great significance or ceremony, but it is implied that they left soon after a couple of events - Harry told Hedwig (his owl) about their imminent departure and Dudley (Harry's cousin), Aunt Petunia, Uncle Vernon, and himself were leaving the house. The text doesn't specify how he physically exited the house. However, it is mentioned that as they approached the lifts in the Atrium to leave, Harry had misgivings about being intercepted due to their arrival with a silver stag (Patronus) and several other people, which could draw attention."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_pdf(\" how did harry leave the dursley's house for the last time in the book?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
